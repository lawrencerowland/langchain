{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce67db3-6d0f-4958-bb1d-ba4770f91149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"4f7b09ec862884ee6656e8994c13b93c34f8406b52d414f07c4b3e2e8ac739b0\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"af330b1d-e990-4975-ba4d-415e2f8195c4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204d97c-df6a-4dda-a77a-bfc19607156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question answering over documents consists of four steps:\n",
    "#1A Create an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db0e37-0e95-4550-b4dd-805cee4e6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pinecone-client tiktoken openai langchain # run this when in github code spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf68b91-ba09-43eb-9aa7-631220ee197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ee5c3-2c26-400a-91f0-fd8202c45134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pinecone\n",
    "import pinecone\n",
    "pinecone.init(\n",
    "    api_key= os.environ['PINECONE_API_KEY'],\n",
    "    environment=\"us-west1-gcp-free\"  # next to api key in console\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4a808-e945-420b-a570-d21242db13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e87bdc-e4ed-45b6-9ad7-db920ea725ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"langchain-demo1\"  # the name within Pinecone, whether it is a new or old index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87deda37-e58c-48df-91a4-d86ee7fedd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to embed a new document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d595731",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#New_input = TextLoader(r'C:\\Users\\User\\Downloads\\state_of_the_union.txt', encoding='utf-8')\n",
    "#New_input = TextLoader(r'C:\\Users\\User\\iCloudDrive\\00 Motivation or use case or explanation\\00 Context and culture and language\\2008 Microsoft paper on portfolio management dull skimmed definitions.txt')\n",
    "#New_input = TextLoader(r'C:\\Users\\User\\iCloudDrive\\40 Data\\Why American Costs Are So High for rail.txt')\n",
    "New_input = TextLoader(r'C:\\Users\\User\\iCloudDrive\\40 Data\\2014 TfL themes and challenges LR.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e7806-3086-4c6c-aedb-8aa7939e4376",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = New_input.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "New_input_as_chunks= text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f0f75-25b3-4fc4-989c-4626a761ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a new variable for a vector store associated with an existing or new index\n",
    "Vector_store2 = Pinecone.from_documents(New_input_as_chunks, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a84e8-148d-47ac-86f4-953a93ed1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector_store2.add_texts([\"TfL has zero of resource challenges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28312a1f-58d7-469c-93ff-46d59e88cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add docs or text to a particular namespace in an existing index\n",
    "# Vector_store2.add_texts([\"TfL is bit old\"], namespace='narrative')\n",
    "#Vector_store2 = Pinecone.from_documents(New_input_as_chunks, embeddings, index_name=index_name, namespace='narrative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46917533-c1de-42cb-ac16-7537ba466582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1C test adding in text w meta data. Can also do documents but not figured out\n",
    "Vector_store2.add_texts([\"TfL has no resource challenges\"], metadatas=[{\"source\": \"Ronny\"},{\"truthfullness\": \"false\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f413a31-caad-4062-8736-1ad91d2933e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1B basic tests of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "943d3b94-7ccb-403e-b0c4-3ea21cd654be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have not just created the index, load one like this\n",
    "Vector_store1 = Pinecone.from_existing_index(index_name, embeddings,namespace='narrative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "060f787c-29a3-456d-9b03-1cd469b01537",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is TfL like ?\"\n",
    "docs = Vector_store1.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fa659d9-3205-437c-9d1a-de608e206019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='TfL is bit goofy' metadata={}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])   #or print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4becb460-208e-4de6-83be-cead8a0be154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Create a Retriever from that index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "185eafe6-b439-4bfd-aff0-4019597998f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_VS1 = Vector_store1.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f756934-86a7-4b2f-a24b-825b8607b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccdfbed2-38a5-4063-a6ed-d8bc64bd1eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/langchain/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/workspaces/langchain/langchain/llms/openai.py:687: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"gpt-4\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b200f42-3b5c-4259-97a3-719fedef4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Create a question answering chain\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever_VS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe25d885-7603-47b3-a36a-2d8076892553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Ask questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26b67a-214a-4ede-93f9-c86a0bde1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What should i do about asset condition for success\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9bf023-b846-43a4-af73-b1c7403490cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
